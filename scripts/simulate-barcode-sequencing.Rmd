# Setting up functions to make simulations easier

```{r,libraries}
library("tidyverse")
library("stringdist")
library("parallel")
library("igraph")
cl <- makeCluster( detectCores()-1 )
```

Reasonable error rates from Schrimer et al 2016 for a HiSeq 2000 are:

- 5e-4 substitution per base
- 5e-6 insertion and deletion rate per base

NextSeq etc are probably worse than that.

```{r,some-functions,cache=T}
makeDegenerateBarcode <- function(barcode_length=4,
        base_mix=c("A"=1,"T"=1,"C"=1,"G"=1)) {
    paste0(
        sample(names(base_mix),barcode_length,
            prob=base_mix,replace=T
            ),
        collapse=""
        )
}
makeDegenerateBarcode()

substitution_error <- function(x,subvector=list(
            "A"=c("T","C","G"),
            "T"=c("C","G","A"),
            "C"=c("G","A","T"),
            "G"=c("A","T","C"))
        ) {
    base_index <- sample(1:nchar(x),1)
    substr(x,base_index,base_index) <- 
        subvector[[substr(x,base_index,base_index)]][sample(1:3,1)]
    return(x)
}
#substitution_error("ATCATAG")

insertion_error <- function(x) {
    base_index <- sample(1:nchar(x)+2,1)
    return(paste0(substr(x,0,base_index-1),
            sample(c("A","T","C","G"),1),
            substr(x,base_index,nchar(x)),collapse=""))
}
#insertion_error("ATCATAG")

deletion_error <- function(x) {
    base_index <- sample(1:nchar(x),1)
    return(paste0(substr(x,1,base_index-1),
            substr(x,base_index+1,nchar(x)),collapse=""))
}
#deletion_error("ATCATAG")

applyMutations <- function(Reads,Operations,Operation){
    while (Operations > 0) {
        Operations <- Operations - 1
        this_seq <- base::sample(names(Reads),size=1,prob=Reads)
        this_seq
        Reads[this_seq] <- Reads[this_seq] - 1
        mut_seq <- Operation(this_seq)
        if (is.na(Reads[mut_seq])) { 
            Reads[mut_seq] <- 1
        } else {
            Reads[mut_seq] <- Reads[mut_seq] + 1
        }
    }
    return(Reads)
}

makeFakeData <- function(number_barcodes=10,
        barcode_frequencies=rep(1,10),
        barcode_length=4,
        error_rate_substitution=1e-1,
        error_rate_insertion=0,#1e-1,
        error_rate_deletion=0,#1e-1,
        sequencing_depth=1e3,
        cluster=cl
        ) {
    clusterExport(cl,c("substitution_error","insertion_error",
            "deletion_error","applyMutations","tibble"
            ),
        envir=.GlobalEnv)
    tibble(
        Ideal=unlist(replicate(number_barcodes,
                makeDegenerateBarcode(barcode_length=barcode_length),
                simplify=F)
            ),
        Abundance=round(prop.table(barcode_frequencies)*
                    sequencing_depth
                ),
        BarcodeLength=barcode_length,
        ) %>% 
        group_by(Ideal,BarcodeLength) %>%
        summarize(Abundance=sum(Abundance)) %>% # in case collisions
        mutate(Reads=map2(Abundance,Ideal,setNames)) %>%
        group_by(Ideal) %>%
        mutate(Substitutions=rpois(1,lambda=Abundance*error_rate_substitution*BarcodeLength)) %>%
        mutate(Insertions=rpois(1,lambda=Abundance*error_rate_insertion*BarcodeLength)) %>%
        mutate(Deletions=rpois(1,lambda=Abundance*error_rate_deletion*BarcodeLength)) %>%
        rowwise() %>% 
        mutate(Reads=clusterMap(cluster,
                fun=applyMutations,Reads=list(Reads),
                Operations=Substitutions,
                MoreArgs=list(Operation=substitution_error),
                RECYCLE=F,SIMPLIFY=F,USE.NAMES=F
                )
            ) %>% 
        mutate(Reads=clusterMap(cluster,
                fun=applyMutations,Reads=list(Reads),
                Operations=Insertions,
                MoreArgs=list(Operation=insertion_error),
                RECYCLE=F,SIMPLIFY=F,USE.NAMES=F
                )
            ) %>% 
        mutate(Reads=clusterMap(cluster,
                fun=applyMutations,Reads=list(Reads),
                Operations=Deletions,
                MoreArgs=list(Operation=deletion_error),
                RECYCLE=F,SIMPLIFY=F,USE.NAMES=F
                )
            ) %>% 
        mutate(Reads=map(list(Reads),
            function(x){ tibble(Seq=names(x),Count=x)} )
            )%>% 
        select(-Substitutions,-Insertions,-Deletions,-BarcodeLength)%>% 
        unnest(Reads) %>% filter(Count > 0)
}

```


```{r,test-generation-of-fake-data,cache=T}
set.seed(123)
z <- makeFakeData(number_barcodes=1e2,
    barcode_frequencies=rep(1,1e2),
    barcode_length=6,
    error_rate_substitution=5e-3,
    error_rate_insertion=0,#5e-5,
    error_rate_deletion=0,#5e-5,
    sequencing_depth=1e5,
    cluster=cl
    ) 
hist(unlist(lapply(z$Seq,nchar)))
hist(log10(z$Count))
```

With these generation functions, we can make and mutate libraries
with control over the 

`number_barcodes`
 ~ Number to generate, for degenerate libraries you approximate this
    by sampling from a denegerate pool.

`barcode_frequencies`
 ~ This is a vector of relative frequencies of each random barcode.
    This need not sum to 1.

`barcode_length`
 ~ How long are they initially?

`error_rate_substitution`
 ~ From Schrimer et al 2016, recommend using a 5e-4 per base 
    substitution rate.

`error_rate_insertion`
 ~ From Schrimer et al 2016, recommend using a 5e-6 per base 
    insertion rate.

`error_rate_deletion`
 ~ From Schrimer et al 2016, recommend using a 5e-6 per base 
    deletion rate.

`sequencing_depth`
 ~ How many total reads? This isn't really "depth" per se, but it's
    total reads.

Let's make a big data set.

```{r,generate-test-dataset,message=F,cache=T,eval=T}
set.seed(123)
fakedatar_list <- list()
for (replicates in c(1,2,3)) {
 for (ncodes in c(1e1,1e2,1e3)) {
  for (seqdepth in c(1e4,1e5)) {
   freqdists <- list(
        cube_pois=rpois(ncodes,lambda=seqdepth/ncodes)^3,
        pois=rpois(ncodes,lambda=seqdepth/ncodes),
        uniform=rep(1,ncodes)
        )
   for (freqdist in names(freqdists)) {
    for (barlength in c(6,8,10)) {
     for (subrate in c(5e-4)) {
      for (insrate in 0) { #c(5e-6)) {
       for (delrate in 0) { #c(5e-6)) {
        message(length(fakedatar_list)+1)
        fakedatar_list[[length(fakedatar_list)+1]] <- 
            makeFakeData(
                number_barcodes=ncodes,
                barcode_frequencies=freqdists[[freqdist]],
                barcode_length=barlength,
                error_rate_substitution=subrate,
                error_rate_insertion=insrate,
                error_rate_deletion=delrate,
                sequencing_depth=seqdepth,
                cluster=cl
                ) %>%
                mutate(
                    number_barcodes=ncodes,
                    barcode_frequencies=freqdist,
                    barcode_length=barlength,
                    error_rate_substitution=subrate,
                    error_rate_insertion=insrate,
                    error_rate_deletion=delrate,
                    sequencing_depth=seqdepth
                    )
} } } } } } } }
fakedatar <- bind_rows(fakedatar_list)
fakedatar
saveRDS(fakedatar,file="../tmp/fakedatar.RData")
```

```{r,prelim-checks-on-fake-data,cache=T,eval=F}
#fakedatar <- readRDS(file="../tmp/fakedatar.RData")
ggplot(fakedatar) + theme_classic()+
    aes(x=Count)+
    geom_histogram()+scale_x_log10()+
    facet_wrap(~barcode_frequencies)
```


```{r,more-fake-datar,cache=T}
this_fake_datar <- expand.grid(
    replicates=c(1,2,3),
    number_barcodes=c(10,100,1000),
    barcode_length=c(4,6,8),
    error_rate_substitution=c(1e-3,1e-4),
    error_rate_insertion=0,
    error_rate_deletion=0,
    sequencing_depth=c(1e4,1e5)
    ) %>%
    rowwise() %>%
    mutate(z=list(tibble(barcode_freq=c("uniform","cube_pois"),
            barcode_frequencies=list(
                rep(1,number_barcodes),
                rpois(number_barcodes,1e5/number_barcodes)
                )
            )
        )) %>%
    unnest(z) %>%
    rowwise() %>% 
    mutate(fakedata=list(makeFakeData(
                number_barcodes=number_barcodes,
                barcode_frequencies=unlist(barcode_frequencies),
                barcode_length=barcode_length,
                error_rate_substitution=error_rate_substitution,
                error_rate_insertion=error_rate_insertion,
                error_rate_deletion=error_rate_deletion,
                sequencing_depth=sequencing_depth,
                cluster=cl
                ))
        )
saveRDS(this_fake_datar,"../tmp/thisfakedatar.RData")
```


```{r,align-analysis-functions,cache=T}
this_fake_datar <- readRDS("../tmp/thisfakedatar.RData")


analyzeError_hamming <- function(datar,max_edit_distance=1) {
    mutate(datar,best_matches=apply(
            stringdistmatrix(datar$Seq,
                unique(datar$Ideal),
                method="hamming",useNames=T),
            1,function(x){
                if(min(x)>max_edit_distance){return(NA)}
                names(which(x==min(x)))
            })) %>% 
        filter(unlist(map(best_matches,~length(.x)==1))) %>%
        unnest() %>%
        filter(!is.na(best_matches)) %>% 
        {full_join(
            group_by(.,Ideal) %>% summarize(RealCount=sum(Count)) %>%
                rename(Sequence=Ideal),
            group_by(.,best_matches) %>% summarize(Measured=sum(Count)) %>%
                rename(Sequence=best_matches),
            by="Sequence")} %>%
        mutate(AbsoluteDifference=abs(Measured-RealCount)) %>%
        mutate(RelativeError=AbsoluteDifference/RealCount)
}

fake_analysis <- this_fake_datar %>%
    mutate(max_mm=list(c(0,1,2))) %>%
    unnest(max_mm,.drop=F) %>%
    rowwise() %>%
    mutate(fakeanalyze=map2(
            list(fakedata),max_mm,
            analyzeError_hamming))

fake_analysis <- fake_analysis %>%
    select(-fakedata,-barcode_frequencies) %>%
    unnest() %>%
    select(-Sequence)

fake_analysis %>% ggplot()+ #theme_classic()+
    facet_grid(number_barcodes+sequencing_depth~error_rate_substitution+barcode_freq+max_mm)+
    aes(x=factor(barcode_length),group=interaction(barcode_length,replicates),y=RelativeError) + 
    geom_violin()+
    scale_y_log10()


```

```{r,graph-analysis-functions,cache=T}
diversify_list <- list(
    "A"=c("A","T","C","G"),
    "T"=c("T","C","G","A"),
    "C"=c("C","G","A","T"),
    "G"=c("G","A","T","C")
    ) 

# This takes a barcode, expands out the mismatch threshold as
# needed to make neighbors, then compares the barcode to the 
# neighbors
spreadBarcodes <- function(this_barcode,mismatch_threshold=1) {
    #
    the_neighbors <- list(this_barcode)
    while( mismatch_threshold > 0 ) {
        mismatch_threshold <- mismatch_threshold - 1
        the_neighbors <- unique(unlist(
            lapply(the_neighbors,
                function(that_barcode){
                    lapply(1:nchar(that_barcode),
                        function(x){
                            apply(expand.grid(
                                    substr(that_barcode,0,x-1),
                                    c("A","T","C","G"),
                                    substr(that_barcode,x+1,nchar(that_barcode))
                                    ),
                                1,function(x){ paste0(x,collapse="") }
                                )
                        }
                        )
                }
                )
            ))
    }
    data.frame(
        query=this_barcode,
        as.list(setNames(
                stringdist(this_barcode,the_neighbors,
                    method="hamming"),
                the_neighbors
                )
            )
        )
}
spreadBarcodes("AAA",2)

# This uses the above function to build an adjacency matrix for some
# supplied barcodes 
buildAdjacency <- function(supplied_barcodes) {
    this_df <- Reduce(function(x,y) merge(x,y,all=T),
        lapply(supplied_barcodes,spreadBarcodes)
        )
    this_df <- merge(
        this_df,
        Reduce(function(x,y) merge(x,y,all=T),
            lapply(names(this_df[,-1]),spreadBarcodes)
            ),
        all=T
        )
    this_matrix <- as.matrix(this_df[,-1])
    rownames(this_matrix) <- this_df[,1]
    this_matrix[is.na(this_matrix)] <- 0
    return(this_matrix)
}
buildAdjacency(c("AAA","GGG"))


adjMatrix <- buildAdjacency(c("AAA","AAT","TAA","TAT"))
adjDF <- as.tibble(adjMatrix) %>%
    mutate(query=rownames(adjMatrix)) %>% 
    gather(to,value,-query) %>%
    filter(value>0)
this_graph <- graph_from_data_frame(adjDF,directed=T)
V(this_graph)$counts <- c("AAA"=10,"TAT"=10,"TAA"=1,"AAT"=2)[V(this_graph)$name]
V(this_graph)$counts[is.na(V(this_graph)$counts)] <- 0
V(this_graph)$size <- V(this_graph)$counts
E(this_graph)$weight <- c("AAA"=10,"TAT"=10,"TAA"=1,"AAT"=2)[head_of(this_graph,E(this_graph))$name]
E(this_graph)$weight[is.na(E(this_graph)$weight)] <- 0
E(this_graph)$arrow.size <- 0
E(this_graph)$width <- E(this_graph)$weight
this_layout <- layout_with_lgl(this_graph)#,weights=E(this_graph)$weight)
plot(this_graph,layout=this_layout)


# TESTING HERE

this_fake_datar <- makeFakeData(number_barcodes=10,
    barcode_frequencies=freqdists[["pois"]][1:10],
    barcode_length=5,
    error_rate_substitution=1e-3, 
    error_rate_insertion=0,
    error_rate_deletion=0,
    sequencing_depth=1e5,
    cluster=cl)

adjMatrix <- buildAdjacency(this_fake_datar$Seq)

adjDF <- as.tibble(adjMatrix) %>%
    mutate(query=rownames(adjMatrix)) %>% 
    gather(to,value,-query) %>%
    filter(value>0) %>%
    left_join(this_fake_datar%>%
            mutate(IsCanonical=Seq==Ideal) %>%
            rename(query=Seq)%>%
            group_by(query,IsCanonical)%>%
            summarize(Count=sum(Count))%>%
            select(query,Count,IsCanonical),
        by="query") %>%
    mutate(Count=ifelse(is.na(Count),0,Count)) %>%
    filter(Count>0) %>%
    mutate(IsCanonical=ifelse(is.na(IsCanonical),F,IsCanonical))
this_graph <- graph_from_data_frame(adjDF,directed=T,
    vertices= distinct(rbind(
        adjDF[,c("query","Count","IsCanonical")],
        data.frame(query=setdiff(adjDF$to,adjDF$query),Count=0,IsCanonical=F)
        ))%>%group_by(query)%>%summarize(Count=max(Count),IsCanonical=any(IsCanonical))
    )
#
V(this_graph)$Count <- V(this_graph)$Count#/max(V(this_graph)$Count)
E(this_graph)$Count <- E(this_graph)$Count#/max(E(this_graph)$Count)
#
V(this_graph)$Count <- log(V(this_graph)$Count)
E(this_graph)$Count <- (E(this_graph)$Count)
#
V(this_graph)$size <- V(this_graph)$Count
E(this_graph)$weight <- E(this_graph)$Count
E(this_graph)$arrow.size <- 0
V(this_graph)$label.cex <- 1e-23
V(this_graph)$color <- V(this_graph)$IsCanonical
E(this_graph)$width <- 1#log(E(this_graph)$weight)
this_layout <- layout_with_fr(this_graph)
plot(this_graph,layout=this_layout)

### TESTING ABOVE

#
#analyzeForLevDist <- function(datar,lev_dists=1:3) {
#    datar <- datar %>% 
#        mutate(LevDist=clusterMap(cl,fun=stringdist,
#                Ideal,Seq,MoreArgs=list(method="lv"),
#                RECYCLE=F,SIMPLIFY=F,USE.NAMES=F
#                )
#            )
#    resultz <- list()
#    for (i in lev_dists) {
#        resultz[[length(resultz)+1]] <- datar %>% 
#            filter(LevDist < i) %>% 
#            mutate(MaxMM=i,
#                SampleTotalCount=sum(Count)
#                ) %>%
#            group_by(Ideal,barcode_length,MaxMM,SampleTotalCount,
#                barcode_frequencies,sequencing_depth,
#                number_barcodes,error_rate_substitution,
#                error_rate_insertion,error_rate_deletion
#                ) %>% 
#            summarize(Abundance=sum(Abundance),
#                CountPerIdeal=sum(Count)
#                ) %>%
#            group_by(barcode_length,MaxMM,SampleTotalCount,
#                barcode_frequencies,sequencing_depth,
#                number_barcodes,error_rate_substitution,
#                error_rate_insertion,error_rate_deletion
#                ) %>% 
#            summarize(SummaryAbsError=list(broom::tidy(
#                    summary(na.omit(abs(CountPerIdeal-Abundance)/Abundance))
#                    ))
#                ) %>% 
#            unnest(SummaryAbsError)
#    }
#    return(bind_rows(resultz))
#}
#z <- analyzeForLevDist(fakedatar[1:100,],lev_dists=1)
#```
#
#```{r,apply-that-lev-func,cache=T}
#fakedatar_lev <- analyzeForLevDist(fakedatar,lev_dists=1:3)
#saveRDS(fakedatar_lev,file="levd.RData")
#```
#
#```{r,analyze-levd-datar,cache=T,fig.width=12,fig.height=8}
##fakedatar_lev <- readRDS(file="levd.RData")
#g <- fakedatar_lev%>% ggplot()+theme_minimal()+
#    aes(x=factor(sequencing_depth))+
#    geom_boxplot(stat="identity",aes(ymin=minimum,lower=q1,
#            middle=median,upper=q3,ymax=maximum)
#        )+
#    scale_y_log10()+
#    geom_point(aes(y=mean))+
#    facet_grid(error_rate_substitution+MaxMM~barcode_frequencies+barcode_length+number_barcodes)+
#    ggtitle("Top is barcodes and frequency distribution,\nside is error rate and tolerated leveshtein distance")+
#    ylab("Absolute error over the known\n abs(x^ - x)/x")+
#    xlab("Sequencing depth")#+ scale_y_continuous(limits=c(0,1))
#g
#
#```
#
#
#
#```{r,eval=F,echo=F}
#```{r,for_as_script,eval=F,echo=F}
##!/usr/bin/env Rscript
#args = commandArgs(trailingOnly=TRUE)
## test if there is at least one argument: if not, return an error
#if (length(args)==0) {
#  stop("At least one argument must be supplied (input file).\n", call.=FALSE)
#} else if (length(args)==1) {
#  # default output file
#  args[2] = "out.txt"
#}
#```
#
#```{r}
#library(dada2)
#```
#
#
#```
#
#
#```{r,for_as_script,eval=F,echo=F}
##!/usr/bin/env Rscript
#args = commandArgs(trailingOnly=TRUE)
## test if there is at least one argument: if not, return an error
#if (length(args)==0) {
#  stop("At least one argument must be supplied (input file).\n", call.=FALSE)
#} else if (length(args)==1) {
#  # default output file
#  args[2] = "out.txt"
#}
#```
#
#```{r}
#library(dada2)
#```
#
#Starting with the QC stuff they do in the tutorial.
#
#```{r,filez}
#filezF <- "../tmp/mini_barcodes01_conformed_F_pass.fastq"
#filezR <- "../tmp/mini_barcodes01_conformed_R_pass.fastq"
#```
#
#```{r,qc}
#plotQualityProfile(c(filezF,filezR))
#```
#
#```{r,lern}
#erz <- list()
#plotz <- list()
#for (each in c(1,2) ) {
#    erz[[each]] <- list()
#    plotz[[each]] <- list()
#    for (i in c(1e-25,1e-20,1e-15,1e-10,1e-5)) {
#        erz[[each]][[i]] <- learnErrors(list(filezF,filezR)[each],
#            multithread=TRUE,nbases=1e5,MAX_CONSIST=20,OMEGA_C=i)
#        plotz[[each]][[i]] <- plotErrors(erz[[each]][[i]], nominalQ=TRUE)
#        print(plotz[[each]][[i]])
#    }
#}
#
#i <- 1e-10
#erz[[1]][[i]] <- learnErrors(filezF,
#            multithread=TRUE,nbases=1e5,MAX_CONSIST=20,OMEGA_C=i)
#plotz[[1]] <- list()
#plotz[[each]][[i]] <- plotErrors(erz[[each]][[i]], nominalQ=TRUE)
#
#```
#
#
#```{r,derep}
#derepFs <- derepFastq(filezF, verbose=TRUE)
#derepRs <- derepFastq(filezR, verbose=TRUE)
## Name the derep-class objects by the sample names
#names(derepFs) <- sample.names
#names(derepRs) <- sample.names
#```
```
